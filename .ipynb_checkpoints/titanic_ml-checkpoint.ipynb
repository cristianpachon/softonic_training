{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from google.datalab.ml import TensorBoard\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loadind the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv('titanic_data.csv')\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* survival: Survival (0 = No; 1 = Yes)\n",
    "* class: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "* name: Name\n",
    "* sex: Sex\n",
    "* age: Age\n",
    "* sibsp: Number of Siblings/Spouses Aboard\n",
    "* parch: Number of Parents/Children Aboard\n",
    "* ticket: Ticket Number\n",
    "* fare: Passenger Fare\n",
    "* cabin: Cabin\n",
    "* embarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total of rows\n",
    "print(len(df_input))\n",
    "\n",
    "# How many of them survived?\n",
    "print(df_input.Survived.sum())\n",
    "\n",
    "# How many of them did not survive?\n",
    "print(df_input.Survived.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of survivors by gender\n",
    "df_input.groupby(['Sex'])['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of survivors by gender\n",
    "df_input.groupby(['Sex']).agg({'PassengerId': 'count', 'Survived':['sum', 'mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of survivors by gender and class\n",
    "df_input.groupby(['Sex', 'Pclass']).agg({'PassengerId': 'count', 'Survived':['sum', 'mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for age distribution\n",
    "df_input.Age.plot.hist(bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is a model trained?**\n",
    "\n",
    "    - Weights\n",
    "    - Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) \n",
    "\n",
    "# Spliting into train and test\n",
    "msk = np.random.rand(len(df_input)) < 0.8\n",
    "print(msk[1:10])\n",
    "print(\"Total True values: \" + str(sum(msk)))\n",
    "print(\"Total number of rows: \" + str(len(msk)))\n",
    "print(\"Rate of True values: \" + str(1.0*sum(msk)/len(msk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "df_train = df_input[msk]\n",
    "\n",
    "# Evaluation\n",
    "df_evaluation = df_input[~msk]\n",
    "\n",
    "print(\"Train sample size is \" + str(len(df_train)) + \" and Evaluation sample size is: \" + str(len(df_evaluation)))\n",
    "print(\"Together: \" + str(len(df_train) + len(df_evaluation))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.Survived.mean())\n",
    "print(df_evaluation.Survived.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval input functions to read from Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_input_fn(df, num_epochs, columns):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x = df[columns],\n",
    "        y = df[columns[0]],\n",
    "        batch_size = 40,\n",
    "        num_epochs = num_epochs,\n",
    "        shuffle = True,\n",
    "        queue_capacity = 100\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_input_fn(df, columns):\n",
    "  # When we evaluate the model, we do it just in one epoch. This is important\n",
    "  return tf.estimator.inputs.pandas_input_fn(\n",
    "    x = df[columns],\n",
    "    y = df[columns[0]],\n",
    "    batch_size = 40,\n",
    "    shuffle = False,\n",
    "    queue_capacity = 100\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_input_fn(df):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x = df,\n",
    "        y = None,\n",
    "        batch_size = 128,\n",
    "        shuffle = False,\n",
    "        queue_capacity = 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier with tf.Estimator framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the output directory\n",
    "OUTDIR = './titanic_trained'\n",
    "\n",
    "# Target column name\n",
    "target = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features in a \"tensorflow way\"\n",
    "features = ['Sex']\n",
    "cols =  [target] + features\n",
    "\n",
    "# This is for Sex\n",
    "features_tensorflow = [\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key = features[0],\n",
    "        vocabulary_list = ['female', 'male']\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# start fresh each time\n",
    "shutil.rmtree(\n",
    "  OUTDIR, \n",
    "  ignore_errors = True\n",
    ") \n",
    "\n",
    "model = tf.estimator.LinearClassifier(\n",
    "  feature_columns = features_tensorflow, \n",
    "  model_dir = OUTDIR\n",
    ")\n",
    "\n",
    "model.train(\n",
    "  input_fn = make_train_input_fn(\n",
    "    df = df_train, \n",
    "    num_epochs = 10,\n",
    "    columns = cols\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we need an evaluation dataset?**\n",
    "    \n",
    "    - Model can memorise trainig data. It is not escalable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_crossentropy(model, df, columns):\n",
    "    metrics = model.evaluate(\n",
    "        input_fn = make_eval_input_fn(\n",
    "            df = df,\n",
    "            columns = columns\n",
    "        )\n",
    "    )\n",
    "    print('Cross entropy on dataset = {}'.format(metrics['average_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_crossentropy(model, df_evaluation, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving loss function: Adding more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:,'Pclass_mod'] = df_train['Pclass'] - 1\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation.loc[:, 'Pclass_mod'] = df_evaluation['Pclass'] - 1\n",
    "df_evaluation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values for Age: \" + str(sum(df_train.Age.isnull())))\n",
    "df_train.Age.fillna(df_train.Age.mean(), inplace=True)\n",
    "print(\"Missing values for Age after replacing: \" + str(sum(df_train.Age.isnull())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation.Age.fillna(df_train.Age.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features in a \"tensorflow way\"\n",
    "features = ['Sex', 'Pclass_mod', 'Age']\n",
    "cols =  [target] + features\n",
    "\n",
    "features_tensorflow = [\n",
    "    # This is for Sex\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key = features[0],\n",
    "        vocabulary_list = ['female', 'male']\n",
    "    ),\n",
    "    \n",
    "    # This is for Pclass_mod\n",
    "    tf.feature_column.categorical_column_with_identity(\n",
    "        key = features[1],\n",
    "        num_buckets = 3\n",
    "    ),\n",
    "    \n",
    "    # This is for age\n",
    "    tf.feature_column.numeric_column(\n",
    "        key = features[2]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# start fresh each time\n",
    "shutil.rmtree(\n",
    "  OUTDIR, \n",
    "  ignore_errors = True\n",
    ") \n",
    "\n",
    "model = tf.estimator.LinearClassifier(\n",
    "  feature_columns = features_tensorflow, \n",
    "  model_dir = OUTDIR\n",
    ")\n",
    "\n",
    "model.train(\n",
    "  input_fn = make_train_input_fn(\n",
    "    df = df_train, \n",
    "    num_epochs = 10,\n",
    "    columns = cols\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_crossentropy(model, df_evaluation, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving loss function: Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'classSex'] = df_train['Pclass'].astype(str) + df_train['Sex']\n",
    "df_evaluation.loc[:, 'classSex'] = df_evaluation['Pclass'].astype(str) + df_evaluation['Sex']\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features in a \"tensorflow way\"\n",
    "features = ['classSex', 'Age']\n",
    "cols =  [target] + features\n",
    "\n",
    "features_tensorflow = [\n",
    "    # This is for classSex\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key = features[0],\n",
    "        vocabulary_list = ['1female', '2female', '3female', '1male', '2male', '3male']\n",
    "    ),\n",
    "    \n",
    "    \n",
    "    # This is for age\n",
    "    tf.feature_column.numeric_column(\n",
    "        key = features[1]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# start fresh each time\n",
    "shutil.rmtree(\n",
    "  OUTDIR, \n",
    "  ignore_errors = True\n",
    ") \n",
    "\n",
    "model = tf.estimator.LinearClassifier(\n",
    "  feature_columns = features_tensorflow, \n",
    "  model_dir = OUTDIR\n",
    ")\n",
    "\n",
    "model.train(\n",
    "  input_fn = make_train_input_fn(\n",
    "    df = df_train, \n",
    "    num_epochs = 10,\n",
    "    columns = cols\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_crossentropy(model, df_evaluation, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More metrics\n",
    "    - Why is reporting loss function a bad idea?? Business does not care about it since loss does not contain any business meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is it calculated?**\n",
    "\n",
    "$\\frac{TP+TN}{Total}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing accuracy\n",
    "def print_accuracy(model, df, columns):\n",
    "    metrics = model.evaluate(\n",
    "        input_fn = make_eval_input_fn(\n",
    "            df = df,\n",
    "            columns = columns\n",
    "        )\n",
    "    )\n",
    "    print('Accuracy on dataset = {}'.format(metrics['accuracy']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy(model, df_evaluation, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong with this metric?**\n",
    "    - It is threshold dependant.\n",
    "    - It does not work with unbalanced datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is it calculated?**\n",
    "\n",
    "$\\frac{TP}{\\mbox{Predicted positive}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing accuracy\n",
    "def print_precision(model, df, columns):\n",
    "    metrics = model.evaluate(\n",
    "        input_fn = make_eval_input_fn(\n",
    "            df = df,\n",
    "            columns = columns\n",
    "        )\n",
    "    )\n",
    "    print('Precision on dataset = {}'.format(metrics['precision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_precision(model, df_evaluation, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong with this metric?**\n",
    "- It is threshold dependant.\n",
    "- Model can underpredict positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is it calculated?**\n",
    "\n",
    "$\\frac{TP}{P}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing recall\n",
    "def print_recall(model, df, columns):\n",
    "    metrics = model.evaluate(\n",
    "        input_fn = make_eval_input_fn(\n",
    "            df = df,\n",
    "            columns = columns\n",
    "        )\n",
    "    )\n",
    "    print('Recall on dataset = {}'.format(metrics['recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_recall(model, df_evaluation, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong with this metric?**\n",
    "- It is threshold dependant.\n",
    "- Model can overpredict positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC\n",
    "\n",
    "*The AUC is the probability the model will score a randomly chosen positive class higher than a randomly chosen negative class.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing AUC\n",
    "def print_auc(model, df, columns):\n",
    "    metrics = model.evaluate(\n",
    "        input_fn = make_eval_input_fn(\n",
    "            df = df,\n",
    "            columns = columns\n",
    "        )\n",
    "    )\n",
    "    print('AUC on dataset = {}'.format(metrics['auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_auc(model, df_evaluation, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_df = TensorBoard.list()\n",
    "if not pids_df.empty:\n",
    "    for pid in pids_df['pid']:\n",
    "        TensorBoard().stop(pid)\n",
    "        print('Stopped TensorBoard with pid {}'.format(pid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
